
@article{wagh_modelling_2017,
	title = {Modelling auto insurance claims in Singapore},
	volume = {18},
	issn = {2424-6271, 1391-4987},
	url = {https://sljastats.sljol.info/article/10.4038/sljastats.v18i2.7957/},
	doi = {10.4038/sljastats.v18i2.7957},
	abstract = {Claim frequency data in general insurance may not follow the traditional Poisson distribution when there are many zeros. When the number of observed zeros exceeds the number of expected zeros under the Poisson distribution, extra dispersion appears. This paper summarizes several dispersed and zero-inflated count data models, which are used to handle dispersion and excess zeros. We model the insurance claim count data with excess zeros with these models. We use chi-square goodness-of-fit, to test the validity of the assumption of the count data distribution and fit count data regression model with predictors. We compare the fits through {AIC} and {BIC}. The generalized Poisson model and Negative binomial model provide a good fit to the data.},
	pages = {105},
	number = {2},
	journaltitle = {Sri Lankan Journal of Applied Statistics},
	shortjournal = {Sri Lankan J App Stats},
	author = {Wagh, Yogita S. and Kamalja, Kirtee K.},
	urldate = {2021-10-07},
	date = {2017-12-26},
	langid = {english},
	file = {Wagh and Kamalja - 2017 - Modelling auto insurance claims in Singapore.pdf:files/88/Wagh and Kamalja - 2017 - Modelling auto insurance claims in Singapore.pdf:application/pdf},
}

@article{barnett_best_nodate,
	title = {Best Estimates for Reserves},
	abstract = {In recent years a number of authors (Brosius, 1992; Mack, 1993, 1994; and Murphy, 1994) have shown that link ratio techniques for loss reserving can be regarded as weighted regressions of a certain kind. We extend these regression models to handle different exposure bases and modeling of trends in the incremental data, and develop a variety of diagnostic tools for testing the assumptions these techniques carry with them.},
	pages = {54},
	author = {Barnett, Glen and Zehnwirth, Ben},
	langid = {english},
	file = {Barnett and Zehnwirth - Best Estimates for Reserves.pdf:files/89/Barnett and Zehnwirth - Best Estimates for Reserves.pdf:application/pdf},
}

@article{jong_generalized_nodate,
	title = {{GENERALIZED} {LINEAR} {MODELS} {FOR} {INSURANCE} {DATA}},
	pages = {208},
	author = {Jong, Piet De and Heller, Gillian Z},
	langid = {english},
	file = {Jong and Heller - GENERALIZED LINEAR MODELS FOR INSURANCE DATA.pdf:files/90/Jong and Heller - GENERALIZED LINEAR MODELS FOR INSURANCE DATA.pdf:application/pdf},
}

@article{gross_individual_nodate,
	title = {Individual Claim Development Models and Detailed Actuarial Reserves in Property‐Casualty Insurance},
	pages = {70},
	author = {Gross, Chris G},
	langid = {english},
	file = {Gross - Individual Claim Development Models and Detailed A.pdf:files/91/Gross - Individual Claim Development Models and Detailed A.pdf:application/pdf},
}

@article{dutang_simulation_nodate,
	title = {Simulation of insurance data with actuar},
	pages = {14},
	author = {Dutang, Christophe and Goulet, Vincent},
	langid = {english},
	file = {Dutang and Goulet - Simulation of insurance data with actuar.pdf:files/92/Dutang and Goulet - Simulation of insurance data with actuar.pdf:application/pdf},
}

@article{koissi_emerging_2019,
	title = {Emerging Data Analytics Techniques with Actuarial Applications},
	abstract = {Data analytics strongly rely on data and available computing tools. Recent years have seen an increase in data availability and volume. Advanced computational methods and machine-learning tools have been developed to handle this continuous flow of valuable information. The aim of this research is to survey emerging data analytics techniques and discuss their evolution and growing use in the actuarial profession. Data analytics’ applications in life and non-life insurance will also be provided.},
	pages = {60},
	author = {Koissi, Marie-Claire},
	date = {2019},
	langid = {english},
	file = {Koissi - 2019 - Emerging Data Analytics Techniques with Actuarial .pdf:files/93/Koissi - 2019 - Emerging Data Analytics Techniques with Actuarial .pdf:application/pdf},
}

@article{sajewski_final_nodate,
	title = {Final 2020 Actuarial Value Calculator Methodology},
	pages = {25},
	author = {Sajewski, Sarah},
	langid = {english},
	file = {Sajewski - Final 2020 Actuarial Value Calculator Methodology.pdf:files/94/Sajewski - Final 2020 Actuarial Value Calculator Methodology.pdf:application/pdf},
}

@article{pentikainen_simulation_nodate,
	title = {A {SIMULATION} {PROCEDURE} {FOR} {COMPARING} {DIFFERENT} {CLAIMS} {RESERVING} {METHODS}},
	abstract = {The estimation of outstanding claims is one of the important aspects in the management of the insurance business. Various methods have been widely dealt with in the actuarial literature. Exploration of the inaccuracies involved is traditionally based on a post-facto comparison of the estimates against the actual outcomes of the settled claims. However, until recent years it has not been usual to consider the inaccuracies inherent in claims reserving in the context of more comprehensive (risk theoretical) models, the purpose of which is to analyse the insurer as a whole. Important parts of the technique which will be outlined in this paper can be incorporated into over-all risk theory models to introduce the uncertainty involved with technical reserves as one of the components in solvency and other analyses ({PENTIKAINENet} al. (1989)).},
	pages = {26},
	author = {Pentikainen, Teivo and Rantala, Jukka},
	langid = {english},
	file = {Pentikainen and Rantala - A SIMULATION PROCEDURE FOR COMPARING DIFFERENT CLA.pdf:files/95/Pentikainen and Rantala - A SIMULATION PROCEDURE FOR COMPARING DIFFERENT CLA.pdf:application/pdf},
}

@article{pettifer_practical_nodate,
	title = {A Practical Guide to Commercial Insurance Pricing},
	abstract = {The purpose of this paper is to provide practical guidance to actuaries currently involved or looking to be involved in Commercial insurance pricing to ensure that their work is targeted so that it delivers more effective business outcomes. This includes recommendations on developing knowledge about the portfolio and the wider market, the need to engage with the business and the application of appropriate actuarial technical pricing methods to Commercial insurance.},
	pages = {41},
	author = {Pettifer, Prepared Alina and Pettifer, James},
	langid = {english},
	file = {Pettifer and Pettifer - A Practical Guide to Commercial Insurance Pricing.pdf:files/96/Pettifer and Pettifer - A Practical Guide to Commercial Insurance Pricing.pdf:application/pdf},
}

@article{whitson_unpaid_nodate,
	title = {{UNPAID} {LOSS} \& {ALAE}: {RANGES} {AND} {PERCENTILES}},
	pages = {50},
	author = {Whitson, Scott},
	langid = {english},
	file = {Whitson - UNPAID LOSS & ALAE RANGES AND PERCENTILES.pdf:files/98/Whitson - UNPAID LOSS & ALAE RANGES AND PERCENTILES.pdf:application/pdf},
}

@article{goldburd_generalized_nodate,
	title = {{GENERALIZED} {LINEAR} {MODELS} {FOR} {INSURANCE} {RATING} Second Edition},
	pages = {122},
	author = {Goldburd, Mark and Khare, Anand and Tevet, Dan and Guller, Dmitriy},
	langid = {english},
	file = {Goldburd et al. - GENERALIZED LINEAR MODELS FOR INSURANCE RATING Sec.pdf:files/99/Goldburd et al. - GENERALIZED LINEAR MODELS FOR INSURANCE RATING Sec.pdf:application/pdf},
}

@article{lekhak_relational_nodate,
	title = {Relational Database Design with an Auto Insurance Database Sample},
	pages = {5},
	author = {Lekhak, Prakash},
	langid = {english},
	file = {Lekhak - Relational Database Design with an Auto Insurance .pdf:files/100/Lekhak - Relational Database Design with an Auto Insurance .pdf:application/pdf},
}

@article{badounas_loss_2020,
	title = {Loss Reserving Estimation With Correlated Run-Off Triangles in a Quantile Longitudinal Model},
	volume = {8},
	issn = {2227-9091},
	url = {https://www.mdpi.com/2227-9091/8/1/14},
	doi = {10.3390/risks8010014},
	abstract = {In this paper, we consider a loss reserving model for a general insurance portfolio consisting of a number of correlated run-off triangles that can be embedded within the quantile regression model for longitudinal data. The model proposes a combination of the between- and within-subportfolios (run-off triangles) estimating functions for regression parameter estimation, which take into account the correlation and variation of the run-off triangles. The proposed method is robust to the error correlation structure, improves the efﬁciency of parameter estimators, and is useful for the estimation of the reserve risk margin and value at risk ({VaR}) in actuarial and ﬁnance applications.},
	pages = {14},
	number = {1},
	journaltitle = {Risks},
	shortjournal = {Risks},
	author = {Badounas, Ioannis and Pitselis, Georgios},
	urldate = {2021-10-12},
	date = {2020-02-03},
	langid = {english},
	file = {Badounas and Pitselis - 2020 - Loss Reserving Estimation With Correlated Run-Off .pdf:files/62/Badounas and Pitselis - 2020 - Loss Reserving Estimation With Correlated Run-Off .pdf:application/pdf},
}

@article{parodi_triangle-free_2014,
	title = {Triangle-free reserving A non-traditional framework for estimating reserves and reserve uncertainty ‐ Abstract of the London discussion},
	volume = {19},
	issn = {1357-3217, 2044-0456},
	url = {https://www.cambridge.org/core/product/identifier/S1357321713000354/type/journal_article},
	doi = {10.1017/S1357321713000354},
	abstract = {This abstract relates to the following paper:
            
              
                
                  Parodi
                  P.
                
                Triangle free reserving.
                British Actuarial Journal
                , doi:
                10.1017/S1357321713000093},
	pages = {219--233},
	number = {1},
	journaltitle = {British Actuarial Journal},
	shortjournal = {Br. Actuar. J.},
	author = {Parodi, Pietro},
	urldate = {2021-10-12},
	date = {2014-03},
	langid = {english},
	file = {Parodi - 2014 - Triangle-free reserving A non-traditional framewor.pdf:files/87/Parodi - 2014 - Triangle-free reserving A non-traditional framewor.pdf:application/pdf},
}

@article{gabrielli_individual_nodate,
	title = {Individual Claims Simulation Machine: Description of the R Package},
	abstract = {The R ﬁles Functions.V1 and Simulation.Machine.V1 serve at simulating individual claims cash ﬂows of non-life insurance claims for a synthetic insurance portfolio.},
	pages = {3},
	author = {Gabrielli, Andrea and Wüthrich, Mario V},
	langid = {english},
	file = {Gabrielli and Wüthrich - Individual Claims Simulation Machine Description .pdf:files/144/Gabrielli and Wüthrich - Individual Claims Simulation Machine Description .pdf:application/pdf;Gabrielli and Wüthrich - Individual Claims Simulation Machine Description .pdf:files/154/Gabrielli and Wüthrich - Individual Claims Simulation Machine Description .pdf:application/pdf},
}

@article{stanard_simulation_nodate,
	title = {A {SIMULATION} {TEST} {OF} {PREDICTION} {ERRORS} {OF} {LOSS} {RESERVE} {ESTIMATION} {TECHNIQUES}},
	abstract = {This paper uses a computer simulation model to measure the expected value and variance of prediction errors of four simple methods of estimating loss reserves. Two of these methods are new to the Proceedings.},
	pages = {25},
	author = {Stanard, James N},
	langid = {english},
	file = {Stanard - A SIMULATION TEST OF PREDICTION ERRORS OF LOSS RES.pdf:files/146/Stanard - A SIMULATION TEST OF PREDICTION ERRORS OF LOSS RES.pdf:application/pdf;Stanard - A SIMULATION TEST OF PREDICTION ERRORS OF LOSS RES.pdf:files/151/Stanard - A SIMULATION TEST OF PREDICTION ERRORS OF LOSS RES.pdf:application/pdf},
}

@article{murphy_unbiased_nodate,
	title = {Unbiased Loss Development Factors},
	abstract = {Casualty Actuarial Society literature is inconclusive regarding whether the loss development technique is biased or unbiased, or which of the traditional methods of estimating link ratios is best. This paper presents u mathematical framework {IO} answer those questionsfor the class of linear link ratio estimators used in practice. A more accurate method of calculating link ratios is derived based on classical regression theory. The circumstances under which the traditional methods could be considered optimal are discussed. It is shown that two traditional estimators may in fact be least squares estimators depending on the set of assumptions one believes governs the process of loss development. Form\&s for variances ox and confidence intervals around, point estimates of ultimate loss and loss reserves are derived. A triangle of incurred loss dollars is analyzed to demonstrate the concepts and techniques. A summary of a simulation study is presented and suggests that the performance of the incurred loss development technique based on the more general least squares estimator may approach that of the Bomhuetter-Ferguson and Stanard-Buhlmann techniques in some situations. The requisite mathematics is within the reach of the actuarial student equipped with the first three exams.},
	pages = {64},
	author = {Murphy, Daniel M},
	langid = {english},
	file = {Murphy - Unbiased Loss Development Factors.pdf:files/148/Murphy - Unbiased Loss Development Factors.pdf:application/pdf},
}

@article{adesina_using_2018,
	title = {{USING} R {FOR} {ACTUARIAL} {ANALYSIS} {IN} {VALUATION} {AND} {RESERVING}},
	abstract = {The introduction of R software into the statistical computing space has provided comprehensive language for managing and manipulating multidimensional data. Developing the capacity and skills of students and actuarial analysts is essential for actuarial practices. In this study, the use of R is proposed as a decision support tool for in the field of actuarial teaching and practice, as a complement to the existing excel and other existing platform. Count data was fitted using six regression models, out of which zero-inflated Poisson model is considered to be most suitable model for the count data based on model selection criteria; also procedure for reserving is demonstrated. It is expected that this would promote the use of R among academia and practitioners.},
	pages = {7},
	author = {Adesina, Olumide Sunday and Dare, Remi Julius and Famurewa, Oludolapo Kehinde},
	date = {2018},
	langid = {english},
	keywords = {R, Actuarial},
	file = {Adesina et al. - 2018 - USING R FOR ACTUARIAL ANALYSIS IN VALUATION AND RE.pdf:files/150/Adesina et al. - 2018 - USING R FOR ACTUARIAL ANALYSIS IN VALUATION AND RE.pdf:application/pdf},
}

@online{alessandro_carrato_fabio_concina_markus_gesmann_dan_murphy_mario_wuthrich_and_chainladder_2021,
	title = {{ChainLadder}: Claims reserving with R},
	url = {https://cran.rstudio.com/web/packages/ChainLadder/vignettes/ChainLadder.html},
	author = {Alessandro Carrato, Fabio Concina, Markus Gesmann, Dan Murphy, Mario Wüthrich and},
	urldate = {2021-10-12},
	date = {2021-10-06},
	keywords = {R, Package, Actuarial},
	file = {ChainLadder\: Claims reserving with R:files/157/ChainLadder.html:text/html},
}

@article{carrato_claims_nodate,
	title = {Claims reserving with R: {ChainLadder}-0.2.5 Package Vignette},
	abstract = {The {ChainLadderpackage} provides various statistical methods which are typically used for the estimation of outstanding claims reserves in general insurance, including those to estimate the claims development results as required under Solvency {II}.},
	pages = {58},
	author = {Carrato, Alessandro and Concina, Fabio and Gesmann, Markus and Murphy, Dan and Wuthrich, Mario and Zhang, Wayne},
	langid = {english},
	file = {Carrato et al. - Claims reserving with R ChainLadder-0.2.5 Package.pdf:files/159/Carrato et al. - Claims reserving with R ChainLadder-0.2.5 Package.pdf:application/pdf},
}

@article{kuo_deeptriangle_2019,
	title = {{DeepTriangle}: A Deep Learning Approach to Loss Reserving},
	volume = {7},
	issn = {2227-9091},
	url = {https://www.mdpi.com/2227-9091/7/3/97},
	doi = {10.3390/risks7030097},
	shorttitle = {{DeepTriangle}},
	abstract = {We propose a novel approach for loss reserving based on deep neural networks. The approach allows for joint modeling of paid losses and claims outstanding, and incorporation of heterogeneous inputs. We validate the models on loss reserving data across lines of business, and show that they improve on the predictive accuracy of existing stochastic methods. The models require minimal feature engineering and expert input, and can be automated to produce forecasts more frequently than manual workﬂows.},
	pages = {97},
	number = {3},
	journaltitle = {Risks},
	shortjournal = {Risks},
	author = {Kuo, Kevin},
	urldate = {2021-10-26},
	date = {2019-09-16},
	langid = {english},
	file = {Kuo - 2019 - DeepTriangle A Deep Learning Approach to Loss Res.pdf:files/170/Kuo - 2019 - DeepTriangle A Deep Learning Approach to Loss Res.pdf:application/pdf},
}

@article{noauthor_loss_nodate,
	title = {Loss Data Analytics},
	pages = {319},
	langid = {english},
	file = {Loss Data Analytics.pdf:files/173/Loss Data Analytics.pdf:application/pdf},
}

@book{community_loss_nodate,
	title = {Loss Data Analytics},
	url = {https://openacttexts.github.io/Loss-Data-Analytics/},
	abstract = {Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience.},
	author = {Community, An open text authored by the Actuarial},
	urldate = {2021-10-26},
	file = {Snapshot:files/177/Loss-Data-Analytics.html:text/html},
}

@article{ritzke_data_nodate,
	title = {Data Warehousing for Actuaries},
	abstract = {Many life insurance companies have some data warehousing project in progress. Much activity in this area has been focused on marketing, sales, and accounting. Less work has been done in financial management areas, but this is expected to grow over time.},
	pages = {21},
	author = {Ritzke, Charles E and Pledge, Kevin J and Raden, Neil},
	langid = {english},
	file = {Ritzke et al. - Data Warehousing for Actuaries.pdf:files/180/Ritzke et al. - Data Warehousing for Actuaries.pdf:application/pdf},
}

@article{popelyukhin_actuarial_nodate,
	title = {The Actuarial Process from a Data Management Point of View},
	pages = {11},
	author = {Popelyukhin, Aleksey S},
	langid = {english},
	file = {Popelyukhin - The Actuarial Process from a Data Management Point.pdf:files/183/Popelyukhin - The Actuarial Process from a Data Management Point.pdf:application/pdf},
}

@article{strube_actuarial_2005,
	title = {Actuarial Data Management In A High-Volume Transactional Processing Environment},
	abstract = {The development and management of data resources that support property/casualty actuarial work are very challenging undertakings, especially in a high-volume transactional processing environment. In order to equip actuaries with the data resources necessary to excel in the performance of their functions, an Actuarial Data Management ({ADM}) support team is needed. It serves as a proactive, added-value conduit of business data and specialized technical support to an actuarial staff. This paper examines the evolution of the actuarial data management function in the context of end user computing, and highlights the key roles and processes that comprise an effective data management operation in a modern property/casualty actuarial department. The paper also includes a case study that describes the development of the data management function in the Actuarial Department of Motors Insurance Corporation, a member of the {GMAC} Insurance Group, located in Southfield, Michigan.},
	pages = {41},
	author = {Strube, Joseph and Russell, Bryant},
	date = {2005},
	langid = {english},
	file = {Strube and Russell - 2005 - Actuarial Data Management In A High-Volume Transac.pdf:files/185/Strube and Russell - 2005 - Actuarial Data Management In A High-Volume Transac.pdf:application/pdf},
}

@article{krumenaker_111-29_nodate,
	title = {111-29: Developing Data Marts and Web-Enabled {OLAP} for Actuarial and Underwriting Analysis},
	abstract = {A data warehouse or data mart is the foundation of an on-line analytical processing ({OLAP}) system used to develop pricing models in the insurance industry. The issues we face in creating such a data mart and multi-dimensional database ({MDDB}) for actuarial and underwriting departments include (1) which factors to evaluate using {OLAP}, i.e., selecting the class variables - e.g., age, experience, points - and analysis variables related to premiums and claims, (2) how to most efficiently and accurately develop {MDDB} cubes from customer transaction records in the data warehouse, and (3) how to empower business users to change business rules without requiring changes to hard code, using popular user-friendly software and a simple syntax. The overall solution must achieve a variety of goals: Minimizing processing time requires efficient code; ensuring accuracy with routines to detect data anomalies throughout the process; maximizing the solution’s analytical value, which requires close consultation with the users and an understanding of their business issues. We can enable business users to manage changes in certain business rules without re-programming by using tools familiar to users (e.g., Microsoft Excel) and the {SAS}® macro language to automatically generate new code during each periodic production run.},
	pages = {7},
	journaltitle = {Data Warehousing},
	author = {Krumenaker, Michael and Bukhbinder, George},
	langid = {english},
	file = {Krumenaker and Bukhbinder - 111-29 Developing Data Marts and Web-Enabled OLAP.pdf:files/186/Krumenaker and Bukhbinder - 111-29 Developing Data Marts and Web-Enabled OLAP.pdf:application/pdf},
}

@article{narayan_anatomy_2010,
	title = {Anatomy of Actuarial Methods of Loss Reserving},
	abstract = {This paper evaluates the foundation of loss reserving methods currently used by actuaries in property casualty insurance. The chain-ladder method, also known as the weighted loss development method in North America, is the most commonly used actuarial technique for loss reserving and setting liabilities for property/casualty insurers. Many actuaries believe that the basic assumption underlying this model is the future development of losses is dependent on losses to date for each accident year. We shall see that this is not the case and the method may be rooted in the complete independence of future loss development. The alternative assumptions are, in this author’s opinion, a more natural way of analyzing the loss triangle. We shall also show that most of the methods used by actuaries are based on one common basic model, and the differences lie in how and which of the parameters are being estimated. The exposition provides some new insight to reserving methods. While it enriches our understanding of the loss reserving process and defines the common thread among various methods, it challenges some commonly held views in the actuarial profession. The exposition here points out a flaw in the Bornhuetter-Ferguson methodology as well as questions the basic framework of the loss development methodology. We shall show that we can obtain the same results as the loss development method under the assumption that the future losses are independent of what we know currently.},
	pages = {23},
	author = {Narayan, Prakash},
	date = {2010},
	langid = {english},
	file = {Narayan - 2010 - Anatomy of Actuarial Methods of Loss Reserving.pdf:files/193/Narayan - 2010 - Anatomy of Actuarial Methods of Loss Reserving.pdf:application/pdf},
}

@article{kuo_deeptriangle_2019-1,
	title = {{DeepTriangle}: A Deep Learning Approach to Loss Reserving},
	volume = {7},
	issn = {2227-9091},
	url = {http://arxiv.org/abs/1804.09253},
	doi = {10.3390/risks7030097},
	shorttitle = {{DeepTriangle}},
	abstract = {We propose a novel approach for loss reserving based on deep neural networks. The approach allows for joint modeling of paid losses and claims outstanding, and incorporation of heterogeneous inputs. We validate the models on loss reserving data across lines of business, and show that they improve on the predictive accuracy of existing stochastic methods. The models require minimal feature engineering and expert input, and can be automated to produce forecasts more frequently than manual workﬂows.},
	pages = {97},
	number = {3},
	journaltitle = {Risks},
	shortjournal = {Risks},
	author = {Kuo, Kevin},
	urldate = {2021-11-15},
	date = {2019-09-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1804.09253},
	keywords = {Computer Science - Machine Learning, Quantitative Finance - Risk Management, Statistics - Applications},
	annotation = {Comment: Published version available at https://www.mdpi.com/2227-9091/7/3/97},
	file = {Kuo - 2019 - DeepTriangle A Deep Learning Approach to Loss Res.pdf:files/197/Kuo - 2019 - DeepTriangle A Deep Learning Approach to Loss Res.pdf:application/pdf},
}